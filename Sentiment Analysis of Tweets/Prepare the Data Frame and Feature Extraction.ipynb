{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample_submission.csv',\n",
       " 'test_data.csv',\n",
       " 'text_emotion.csv',\n",
       " 'train_data.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment       author  \\\n",
       "0  1956967341       empty   xoshayzers   \n",
       "1  1956967666     sadness    wannamama   \n",
       "2  1956967696     sadness    coolfunky   \n",
       "3  1956967789  enthusiasm  czareaquino   \n",
       "4  1956968416     neutral    xkilljoyx   \n",
       "\n",
       "                                             content  \n",
       "0  @tiffanylue i know  i was listenin to bad habi...  \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                Funeral ceremony...gloomy friday...  \n",
       "3               wants to hang out with friends SOON!  \n",
       "4  @dannycastillo We want to trade with someone w...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/text_emotion.csv')\n",
    "print (df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter out the columns we require"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['sentiment', 'content']\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run various feature extraction tools to get a good idea about our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Text Preprocessing\n",
    "#### 1.1. Remove Stop Words and Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_string(s): \n",
    "  \n",
    "    # initialization of string to \"\" \n",
    "    new = \"\" \n",
    "  \n",
    "    # traverse in the string  \n",
    "    for x in s: \n",
    "        new += x +' ' \n",
    "  \n",
    "    # return string  \n",
    "    return new.lower() # converting string to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(sentence):\n",
    "\n",
    "    word_tokens = word_tokenize(sentence) \n",
    "\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "\n",
    "    filtered_sentence = [] \n",
    "\n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w) \n",
    "            \n",
    "    return convert_list_to_string(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def remove_tokens(sentence):\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return convert_list_to_string(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#extract lemma\n",
    "def extract_lemma(sent):\n",
    "    parsed_text = {'word':[], 'lemma':[]}\n",
    "    #for sent in doc.sentences:\n",
    "    for wrd in sent:\n",
    "        #extract text and lemma\n",
    "        parsed_text['word'].append(wrd.text)\n",
    "        parsed_text['lemma'].append(wrd.lemma)\n",
    "    #return a dataframe\n",
    "    return pd.DataFrame(parsed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "def extract_lemma(sent):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    word_tokens = word_tokenize(sent) \n",
    "    \n",
    "    lemmatized_sentence = []\n",
    "    for word in word_tokens:\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word))\n",
    "    \n",
    "    return convert_list_to_string(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Driver Function to Process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(sentence):\n",
    "    return extract_lemma(remove_tokens(remove_stopwords(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'layin n bed headache ughhhh waitin call '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_text(df['content'].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up Vader NLP.\n",
    "#### 2.1. Vader Sentiment Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer_scores(sentence):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    #print(\"{:-<40} {}\".format(sentence, str(score)))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.2, 'neu': 0.8, 'pos': 0.0, 'compound': -0.5423}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analyzer_scores(df['content'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Textblob NLP Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "#nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_textblob_features(sentence):\n",
    "    \n",
    "    blob = TextBlob(process_text(sentence))\n",
    "    \n",
    "    textblob_dict = {}\n",
    "    \n",
    "    ## Number of noun phrases in the sentence\n",
    "    textblob_dict['Number of Noun Phrases'] = len(list(blob.noun_phrases))\n",
    "    \n",
    "    ## Number of POS tags in the sentence\n",
    "    textblob_dict['Number of POS tags'] = len(list(blob.pos_tags))\n",
    "    \n",
    "    ## Polarity of the sentence\n",
    "    textblob_dict['Sentence Polarity'] = blob.sentiment_assessments.polarity\n",
    "    \n",
    "    ## Polarity of the sentence\n",
    "    textblob_dict['Sentence Subjectivity'] = blob.sentiment_assessments.polarity\n",
    "    \n",
    "    ## Number of words in the processed sentence\n",
    "    textblob_dict['Number of words'] = len(list(blob.words))\n",
    "    \n",
    "    ## Language of the sentence\n",
    "    textblob_dict['Language Detected'] = blob.detect_language()\n",
    "    \n",
    "    return textblob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Number of Noun Phrases': 1,\n",
       " 'Number of POS tags': 6,\n",
       " 'Sentence Polarity': -0.1,\n",
       " 'Sentence Subjectivity': -0.1,\n",
       " 'Number of words': 6,\n",
       " 'Language Detected': 'en'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_textblob_features(df['content'].iloc[1002])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. spaCy Feature Extraction\n",
    "Ref. https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    entities_dict = {}\n",
    "    for ent in doc.ents:\n",
    "        entities_dict[ent.label_] =  ent.text\n",
    "        \n",
    "    return entities_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ORG': 'Apple', 'GPE': 'U.K.', 'MONEY': '$1 billion'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_entities(\"Apple is looking at buying U.K. startup for $1 billion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. afinn Sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "afinn = Afinn()\n",
    "\n",
    "def afinn_score(sentence):\n",
    "    return afinn.score(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. SentiWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "\n",
    "    #Convert between the PennTreebank tags to simple Wordnet tags\n",
    "\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_sentiment(word,tag):\n",
    "    #returns list of pos neg and objective score. But returns empty list if not present in senti wordnet.\n",
    "\n",
    "\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "        return []\n",
    "\n",
    "    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "    if not lemma:\n",
    "        return []\n",
    "\n",
    "    synsets = wn.synsets(word, pos=wn_tag)\n",
    "\n",
    "    if not synsets:\n",
    "        return []\n",
    "\n",
    "    # Take the first sense, the most common\n",
    "    synset = synsets[0]\n",
    "    swn_synset = swn.senti_synset(synset.name())\n",
    "\n",
    "    return [swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score()]\n",
    "\n",
    "\n",
    "ps = PorterStemmer()\n",
    "words_data = ['this','movie','is','wonderful']\n",
    "words_data = [ps.stem(x) for x in words_data]\n",
    "\n",
    "pos_val = nltk.pos_tag(words_data)\n",
    "senti_val=[ get_sentiment(x,y) for (x,y) in pos_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thi', 'NN'), ('movi', 'NN'), ('is', 'VBZ'), ('wonder', 'JJR')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>processed_sentence</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>vader_compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>tiffanylue know listenin bad habit earlier sta...</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.5423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>layin n bed headache ughhhh waitin call</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>funeral ceremony gloomy friday</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.4767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>want hang friend soon</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.5423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>dannycastillo we want trade someone houston ti...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.0772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content  \\\n",
       "0       empty  @tiffanylue i know  i was listenin to bad habi...   \n",
       "1     sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2     sadness                Funeral ceremony...gloomy friday...   \n",
       "3  enthusiasm               wants to hang out with friends SOON!   \n",
       "4     neutral  @dannycastillo We want to trade with someone w...   \n",
       "\n",
       "                                  processed_sentence  vader_neg  vader_neu  \\\n",
       "0  tiffanylue know listenin bad habit earlier sta...      0.304      0.696   \n",
       "1           layin n bed headache ughhhh waitin call       0.000      1.000   \n",
       "2                    funeral ceremony gloomy friday       0.672      0.328   \n",
       "3                             want hang friend soon       0.000      0.308   \n",
       "4  dannycastillo we want trade someone houston ti...      0.000      0.843   \n",
       "\n",
       "   vader_pos  vader_compound  \n",
       "0      0.000         -0.5423  \n",
       "1      0.000          0.0000  \n",
       "2      0.000         -0.4767  \n",
       "3      0.692          0.5423  \n",
       "4      0.157          0.0772  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Processing the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Processing the text for NLP\n",
    "df['processed_sentence'] = df['content'].apply(lambda x: process_text_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Vader Sentiment Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vader Sentiment Feature Extraction\n",
    "\n",
    "df['vader_neg'] = df['processed_sentence'].apply(lambda x: sentiment_analyzer_scores(x)['neg'])\n",
    "df['vader_neu'] = df['processed_sentence'].apply(lambda x: sentiment_analyzer_scores(x)['neu'])\n",
    "df['vader_pos'] = df['processed_sentence'].apply(lambda x: sentiment_analyzer_scores(x)['pos'])\n",
    "df['vader_compound'] = df['processed_sentence'].apply(lambda x: sentiment_analyzer_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. TextBlob Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Number of Noun Phrases': 1,\n",
       " 'Number of POS tags': 6,\n",
       " 'Sentence Polarity': -0.1,\n",
       " 'Sentence Subjectivity': -0.1,\n",
       " 'Number of words': 6,\n",
       " 'Language Detected': 'en'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_textblob_features(df['content'].iloc[1002])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TextBlob Feature Extraction\n",
    "\n",
    "#try:\n",
    "df['Number of Noun Phrases'] = df['processed_sentence'].apply(lambda x: get_textblob_features(x)['Number of Noun Phrases'])\n",
    "df['Number of POS tags'] = df['processed_sentence'].apply(lambda x: get_textblob_features(x)['Number of POS tags'])\n",
    "df['Sentence Polarity'] = df['processed_sentence'].apply(lambda x: get_textblob_features(x)['Sentence Polarity'])\n",
    "df['Sentence Subjectivity'] = df['processed_sentence'].apply(lambda x: get_textblob_features(x)['Sentence Subjectivity'])\n",
    "df['Number of words'] = df['processed_sentence'].apply(lambda x: get_textblob_features(x)['Number of words'])\n",
    "#except:\n",
    "#    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>processed_sentence</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>vader_compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>tiffanylue know listenin bad habit earlier sta...</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.5423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>layin n bed headache ughhhh waitin call</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>funeral ceremony gloomy friday</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.4767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>want hang friend soon</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.5423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>dannycastillo we want trade someone houston ti...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.0772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content  \\\n",
       "0       empty  @tiffanylue i know  i was listenin to bad habi...   \n",
       "1     sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2     sadness                Funeral ceremony...gloomy friday...   \n",
       "3  enthusiasm               wants to hang out with friends SOON!   \n",
       "4     neutral  @dannycastillo We want to trade with someone w...   \n",
       "\n",
       "                                  processed_sentence  vader_neg  vader_neu  \\\n",
       "0  tiffanylue know listenin bad habit earlier sta...      0.304      0.696   \n",
       "1           layin n bed headache ughhhh waitin call       0.000      1.000   \n",
       "2                    funeral ceremony gloomy friday       0.672      0.328   \n",
       "3                             want hang friend soon       0.000      0.308   \n",
       "4  dannycastillo we want trade someone houston ti...      0.000      0.843   \n",
       "\n",
       "   vader_pos  vader_compound  \n",
       "0      0.000         -0.5423  \n",
       "1      0.000          0.0000  \n",
       "2      0.000         -0.4767  \n",
       "3      0.692          0.5423  \n",
       "4      0.157          0.0772  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
