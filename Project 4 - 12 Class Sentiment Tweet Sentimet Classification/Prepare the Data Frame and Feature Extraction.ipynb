{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample_submission.csv',\n",
       " 'test_data.csv',\n",
       " 'text_emotion.csv',\n",
       " 'train_data.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment       author  \\\n",
       "0  1956967341       empty   xoshayzers   \n",
       "1  1956967666     sadness    wannamama   \n",
       "2  1956967696     sadness    coolfunky   \n",
       "3  1956967789  enthusiasm  czareaquino   \n",
       "4  1956968416     neutral    xkilljoyx   \n",
       "\n",
       "                                             content  \n",
       "0  @tiffanylue i know  i was listenin to bad habi...  \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                Funeral ceremony...gloomy friday...  \n",
       "3               wants to hang out with friends SOON!  \n",
       "4  @dannycastillo We want to trade with someone w...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/text_emotion.csv')\n",
    "print (df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['empty', 'sadness', 'enthusiasm', 'neutral', 'worry', 'surprise',\n",
       "       'love', 'fun', 'hate', 'happiness', 'boredom', 'relief', 'anger'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter out the columns we require"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['sentiment', 'content']\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run various feature extraction tools to get a good idea about our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'layin n bed headache ughhhh waitin call '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from text_utils.text_processing import process_text\n",
    "\n",
    "process_text(df['content'].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('brown')\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up Vader NLP.\n",
    "#### 2.1. Vader Sentiment Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_utils.vader_feature_extraction import sentiment_analyzer_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.2, 'neu': 0.8, 'pos': 0.0, 'compound': -0.5423}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analyzer_scores(df['content'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Textblob NLP Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Number of Noun Phrases': 1,\n",
       " 'Number of POS tags': 11,\n",
       " 'Sentence Polarity': -0.1,\n",
       " 'Sentence Subjectivity': -0.1,\n",
       " 'Number of words': 11,\n",
       " 'Language Detected': 'en'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from text_utils.text_blob_feature_extraction import get_textblob_features\n",
    "get_textblob_features(df['content'].iloc[1002])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. spaCy Feature Extraction\n",
    "Ref. https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_utils.spacy_feature_extraction import get_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DATE': 'weekend'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_entities(process_text(df['content'].iloc[110]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. afinn Sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "afinn = Afinn()\n",
    "\n",
    "def afinn_score(sentence):\n",
    "    return afinn.score(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\envs\\tanush_data_ops\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "## Processing the text for NLP\n",
    "df['processed_sentence'] = df['content'].apply(lambda x: process_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "v = TfidfVectorizer()\n",
    "x = v.fit_transform(df['processed_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'know listenin bad habit earlier started freakin part '"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['processed_sentence'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39771, 29993)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_array = x.toarray()\n",
    "tf_idf_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_cols = [i for i in range(0,1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf_idf_df = pd.DataFrame(tf_idf_array)[tf_idf_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  0  1  2  3  4  5  6  7  8  ...  990  991  992  993  994  995  996  \\\n",
       "0      0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
       "1      1  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
       "2      2  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
       "3      3  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
       "4      4  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0   \n",
       "\n",
       "   997  998  999  \n",
       "0    0    0    0  \n",
       "1    0    0    0  \n",
       "2    0    0    0  \n",
       "3    0    0    0  \n",
       "4    0    0    0  \n",
       "\n",
       "[5 rows x 1001 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_df = tf_idf_df.reset_index()\n",
    "tf_idf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>processed_sentence</th>\n",
       "      <th>String_Length</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>afinn score</th>\n",
       "      <th>Encoded Targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>know listenin bad habit earlier started freaki...</td>\n",
       "      <td>53</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.5423</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>layin n bed headache ughhhh waitin call</td>\n",
       "      <td>40</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>funeral ceremony gloomy friday</td>\n",
       "      <td>31</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.4767</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>want hang friend soon</td>\n",
       "      <td>22</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>want trade someone houston ticket one</td>\n",
       "      <td>38</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  index   sentiment  \\\n",
       "0        0      0       empty   \n",
       "1        1      1     sadness   \n",
       "2        2      2     sadness   \n",
       "3        3      3  enthusiasm   \n",
       "4        4      4     neutral   \n",
       "\n",
       "                                             content  \\\n",
       "0  @tiffanylue i know  i was listenin to bad habi...   \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2                Funeral ceremony...gloomy friday...   \n",
       "3               wants to hang out with friends SOON!   \n",
       "4  @dannycastillo We want to trade with someone w...   \n",
       "\n",
       "                                  processed_sentence  String_Length  \\\n",
       "0  know listenin bad habit earlier started freaki...             53   \n",
       "1           layin n bed headache ughhhh waitin call              40   \n",
       "2                    funeral ceremony gloomy friday              31   \n",
       "3                             want hang friend soon              22   \n",
       "4             want trade someone houston ticket one              38   \n",
       "\n",
       "   vader_neg  vader_neu  vader_pos  vader_compound  afinn score  \\\n",
       "0      0.333      0.667      0.000         -0.5423         -3.0   \n",
       "1      0.000      1.000      0.000          0.0000         -2.0   \n",
       "2      0.672      0.328      0.000         -0.4767         -3.0   \n",
       "3      0.000      0.308      0.692          0.5423          2.0   \n",
       "4      0.000      0.794      0.206          0.0772          1.0   \n",
       "\n",
       "   Encoded Targets  \n",
       "0                2  \n",
       "1               10  \n",
       "2               10  \n",
       "3                3  \n",
       "4                8  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = df.merge(tf_idf_df, on='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('data/tf_idf_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Processing the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Processing the text for NLP\n",
    "df['processed_sentence'] = df['content'].apply(lambda x: process_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6. Vectorize Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument, Word2VecKeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import Word2VecKeyedVectors\n",
    "word2vecModel = Word2VecKeyedVectors.load_word2vec_format('models/GoogleNews-vectors-negative300.bin',binary=True)\n",
    "\n",
    "def word_to_vec_generation(sent):\n",
    "    word_to_vec_value = 0\n",
    "    for word in sent.split(' '):\n",
    "        try:\n",
    "            word_to_vec_value += word2vecModel.word_vec(word)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return word_to_vec_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_vec_generation(\"I am positive and confident\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "#model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "model = Word2Vec(df['processed_sentence'], min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_vectorizer(sent, model=model):\n",
    "    sent_vec = np.zeros(400)\n",
    "    numw = 0\n",
    "    for w in sent:\n",
    "        try:\n",
    "            sent_vec = np.add(sent_vec, model[w])\n",
    "            numw+=1\n",
    "        except:\n",
    "            pass\n",
    "    return sent_vec / np.sqrt(sent_vec.dot(sent_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also download Google’s pre-trained KeyedVectors instance from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 817\n"
     ]
    }
   ],
   "source": [
    "from text_utils.vectorized_feature_extraction import word_to_vec_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_dict = dict(word_to_vec_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(word_to_vec_generation(\"I am positive and confident\")).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vector_df = pd.DataFrame.from_dict(word2vec_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.732452</td>\n",
       "      <td>0.334229</td>\n",
       "      <td>0.041931</td>\n",
       "      <td>1.127319</td>\n",
       "      <td>-0.543579</td>\n",
       "      <td>0.623535</td>\n",
       "      <td>0.225830</td>\n",
       "      <td>0.089233</td>\n",
       "      <td>0.010010</td>\n",
       "      <td>0.634766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136353</td>\n",
       "      <td>0.716064</td>\n",
       "      <td>-0.833984</td>\n",
       "      <td>0.019043</td>\n",
       "      <td>-0.964111</td>\n",
       "      <td>-0.914551</td>\n",
       "      <td>-0.046997</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.288689</td>\n",
       "      <td>0.014404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.416809</td>\n",
       "      <td>-0.072632</td>\n",
       "      <td>0.142395</td>\n",
       "      <td>0.669678</td>\n",
       "      <td>0.147247</td>\n",
       "      <td>-0.625793</td>\n",
       "      <td>0.644409</td>\n",
       "      <td>-0.916504</td>\n",
       "      <td>0.054077</td>\n",
       "      <td>1.186768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026245</td>\n",
       "      <td>0.881958</td>\n",
       "      <td>-0.459880</td>\n",
       "      <td>-0.206543</td>\n",
       "      <td>-0.295959</td>\n",
       "      <td>-1.415039</td>\n",
       "      <td>-0.945801</td>\n",
       "      <td>-0.119263</td>\n",
       "      <td>0.065437</td>\n",
       "      <td>0.718628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.239258</td>\n",
       "      <td>0.264038</td>\n",
       "      <td>0.050049</td>\n",
       "      <td>0.115112</td>\n",
       "      <td>0.391113</td>\n",
       "      <td>-0.896271</td>\n",
       "      <td>0.286377</td>\n",
       "      <td>-0.540039</td>\n",
       "      <td>0.536133</td>\n",
       "      <td>0.519135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415527</td>\n",
       "      <td>-0.335754</td>\n",
       "      <td>-0.590576</td>\n",
       "      <td>0.402786</td>\n",
       "      <td>0.045166</td>\n",
       "      <td>-0.076660</td>\n",
       "      <td>-1.013916</td>\n",
       "      <td>-0.085327</td>\n",
       "      <td>0.336914</td>\n",
       "      <td>0.074219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.325195</td>\n",
       "      <td>-0.148193</td>\n",
       "      <td>0.141113</td>\n",
       "      <td>0.354492</td>\n",
       "      <td>-0.232178</td>\n",
       "      <td>0.279602</td>\n",
       "      <td>0.336609</td>\n",
       "      <td>-0.078918</td>\n",
       "      <td>0.229843</td>\n",
       "      <td>0.110168</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081543</td>\n",
       "      <td>0.841797</td>\n",
       "      <td>-0.323853</td>\n",
       "      <td>-0.526367</td>\n",
       "      <td>0.043076</td>\n",
       "      <td>-0.348389</td>\n",
       "      <td>0.072933</td>\n",
       "      <td>-0.796387</td>\n",
       "      <td>0.424072</td>\n",
       "      <td>-0.353271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.473877</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>-0.079681</td>\n",
       "      <td>0.543701</td>\n",
       "      <td>0.084473</td>\n",
       "      <td>0.193359</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>-0.173920</td>\n",
       "      <td>-0.229004</td>\n",
       "      <td>0.180908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470093</td>\n",
       "      <td>0.607849</td>\n",
       "      <td>0.026611</td>\n",
       "      <td>-0.168457</td>\n",
       "      <td>0.058105</td>\n",
       "      <td>-0.329346</td>\n",
       "      <td>0.307129</td>\n",
       "      <td>-0.805908</td>\n",
       "      <td>-0.022095</td>\n",
       "      <td>-0.410255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.732452  0.334229  0.041931  1.127319 -0.543579  0.623535  0.225830   \n",
       "1  0.416809 -0.072632  0.142395  0.669678  0.147247 -0.625793  0.644409   \n",
       "2  0.239258  0.264038  0.050049  0.115112  0.391113 -0.896271  0.286377   \n",
       "3  0.325195 -0.148193  0.141113  0.354492 -0.232178  0.279602  0.336609   \n",
       "4  0.473877  0.000977 -0.079681  0.543701  0.084473  0.193359  0.000977   \n",
       "\n",
       "        7         8         9      ...          290       291       292  \\\n",
       "0  0.089233  0.010010  0.634766    ...     0.136353  0.716064 -0.833984   \n",
       "1 -0.916504  0.054077  1.186768    ...     0.026245  0.881958 -0.459880   \n",
       "2 -0.540039  0.536133  0.519135    ...     0.415527 -0.335754 -0.590576   \n",
       "3 -0.078918  0.229843  0.110168    ...    -0.081543  0.841797 -0.323853   \n",
       "4 -0.173920 -0.229004  0.180908    ...     0.470093  0.607849  0.026611   \n",
       "\n",
       "        293       294       295       296       297       298       299  \n",
       "0  0.019043 -0.964111 -0.914551 -0.046997  0.033203  0.288689  0.014404  \n",
       "1 -0.206543 -0.295959 -1.415039 -0.945801 -0.119263  0.065437  0.718628  \n",
       "2  0.402786  0.045166 -0.076660 -1.013916 -0.085327  0.336914  0.074219  \n",
       "3 -0.526367  0.043076 -0.348389  0.072933 -0.796387  0.424072 -0.353271  \n",
       "4 -0.168457  0.058105 -0.329346  0.307129 -0.805908 -0.022095 -0.410255  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_vector_df = vector_df.join(df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.732452</td>\n",
       "      <td>0.334229</td>\n",
       "      <td>0.041931</td>\n",
       "      <td>1.127319</td>\n",
       "      <td>-0.543579</td>\n",
       "      <td>0.623535</td>\n",
       "      <td>0.225830</td>\n",
       "      <td>0.089233</td>\n",
       "      <td>0.010010</td>\n",
       "      <td>0.634766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.716064</td>\n",
       "      <td>-0.833984</td>\n",
       "      <td>0.019043</td>\n",
       "      <td>-0.964111</td>\n",
       "      <td>-0.914551</td>\n",
       "      <td>-0.046997</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.288689</td>\n",
       "      <td>0.014404</td>\n",
       "      <td>empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.416809</td>\n",
       "      <td>-0.072632</td>\n",
       "      <td>0.142395</td>\n",
       "      <td>0.669678</td>\n",
       "      <td>0.147247</td>\n",
       "      <td>-0.625793</td>\n",
       "      <td>0.644409</td>\n",
       "      <td>-0.916504</td>\n",
       "      <td>0.054077</td>\n",
       "      <td>1.186768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.881958</td>\n",
       "      <td>-0.459880</td>\n",
       "      <td>-0.206543</td>\n",
       "      <td>-0.295959</td>\n",
       "      <td>-1.415039</td>\n",
       "      <td>-0.945801</td>\n",
       "      <td>-0.119263</td>\n",
       "      <td>0.065437</td>\n",
       "      <td>0.718628</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.239258</td>\n",
       "      <td>0.264038</td>\n",
       "      <td>0.050049</td>\n",
       "      <td>0.115112</td>\n",
       "      <td>0.391113</td>\n",
       "      <td>-0.896271</td>\n",
       "      <td>0.286377</td>\n",
       "      <td>-0.540039</td>\n",
       "      <td>0.536133</td>\n",
       "      <td>0.519135</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.335754</td>\n",
       "      <td>-0.590576</td>\n",
       "      <td>0.402786</td>\n",
       "      <td>0.045166</td>\n",
       "      <td>-0.076660</td>\n",
       "      <td>-1.013916</td>\n",
       "      <td>-0.085327</td>\n",
       "      <td>0.336914</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.325195</td>\n",
       "      <td>-0.148193</td>\n",
       "      <td>0.141113</td>\n",
       "      <td>0.354492</td>\n",
       "      <td>-0.232178</td>\n",
       "      <td>0.279602</td>\n",
       "      <td>0.336609</td>\n",
       "      <td>-0.078918</td>\n",
       "      <td>0.229843</td>\n",
       "      <td>0.110168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.841797</td>\n",
       "      <td>-0.323853</td>\n",
       "      <td>-0.526367</td>\n",
       "      <td>0.043076</td>\n",
       "      <td>-0.348389</td>\n",
       "      <td>0.072933</td>\n",
       "      <td>-0.796387</td>\n",
       "      <td>0.424072</td>\n",
       "      <td>-0.353271</td>\n",
       "      <td>enthusiasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.473877</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>-0.079681</td>\n",
       "      <td>0.543701</td>\n",
       "      <td>0.084473</td>\n",
       "      <td>0.193359</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>-0.173920</td>\n",
       "      <td>-0.229004</td>\n",
       "      <td>0.180908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.607849</td>\n",
       "      <td>0.026611</td>\n",
       "      <td>-0.168457</td>\n",
       "      <td>0.058105</td>\n",
       "      <td>-0.329346</td>\n",
       "      <td>0.307129</td>\n",
       "      <td>-0.805908</td>\n",
       "      <td>-0.022095</td>\n",
       "      <td>-0.410255</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.732452  0.334229  0.041931  1.127319 -0.543579  0.623535  0.225830   \n",
       "1  0.416809 -0.072632  0.142395  0.669678  0.147247 -0.625793  0.644409   \n",
       "2  0.239258  0.264038  0.050049  0.115112  0.391113 -0.896271  0.286377   \n",
       "3  0.325195 -0.148193  0.141113  0.354492 -0.232178  0.279602  0.336609   \n",
       "4  0.473877  0.000977 -0.079681  0.543701  0.084473  0.193359  0.000977   \n",
       "\n",
       "          7         8         9     ...           291       292       293  \\\n",
       "0  0.089233  0.010010  0.634766     ...      0.716064 -0.833984  0.019043   \n",
       "1 -0.916504  0.054077  1.186768     ...      0.881958 -0.459880 -0.206543   \n",
       "2 -0.540039  0.536133  0.519135     ...     -0.335754 -0.590576  0.402786   \n",
       "3 -0.078918  0.229843  0.110168     ...      0.841797 -0.323853 -0.526367   \n",
       "4 -0.173920 -0.229004  0.180908     ...      0.607849  0.026611 -0.168457   \n",
       "\n",
       "        294       295       296       297       298       299   sentiment  \n",
       "0 -0.964111 -0.914551 -0.046997  0.033203  0.288689  0.014404       empty  \n",
       "1 -0.295959 -1.415039 -0.945801 -0.119263  0.065437  0.718628     sadness  \n",
       "2  0.045166 -0.076660 -1.013916 -0.085327  0.336914  0.074219     sadness  \n",
       "3  0.043076 -0.348389  0.072933 -0.796387  0.424072 -0.353271  enthusiasm  \n",
       "4  0.058105 -0.329346  0.307129 -0.805908 -0.022095 -0.410255     neutral  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_vector_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(final_vector_df['sentiment'])\n",
    "final_vector_df['Encoded Targets'] = le.transform(final_vector_df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_vec = final_vector_df[vector_df.columns.values]\n",
    "y_vec = final_vector_df['Encoded Targets']\n",
    "\n",
    "X_vec_train, X_vec_test, y_vec_train, y_vec_test = train_test_split(X_vec, y_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tanush Pas\\AppData\\Local\\conda\\conda\\envs\\PDF\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.fit(X_vec_train, y_vec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2476"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.score(X_vec_test, y_vec_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7. Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(df['processed_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vec = cv.transform(df['processed_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_dataframe = pd.DataFrame(X_vec.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 29993)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_columns = ['vector_' + str(i) for i in range(0,vectorized_dataframe.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorized_dataframe.columns =  vector_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_dataframe = vectorized_dataframe.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_dataframe.to_csv('data/vectorized_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8. N-Gram Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_vectorizer.fit(df['processed_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ngram_vec = ngram_vectorizer.transform(df['processed_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_ngram_vec, y, train_size = 0.75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tanush Pas\\AppData\\Local\\conda\\conda\\envs\\PDF\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Tanush Pas\\AppData\\Local\\conda\\conda\\envs\\PDF\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Tanush Pas\\AppData\\Local\\conda\\conda\\envs\\PDF\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17597765363128492"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.07      0.20      0.11        10\n",
      "           1       0.38      0.43      0.41        23\n",
      "           2       0.09      0.10      0.10        29\n",
      "           3       0.24      0.19      0.21        37\n",
      "           4       0.23      0.33      0.27        21\n",
      "           5       0.08      0.11      0.09        18\n",
      "           6       0.35      0.38      0.37        29\n",
      "           7       0.25      0.24      0.24        25\n",
      "           8       0.21      0.11      0.14        46\n",
      "           9       0.17      0.17      0.17        29\n",
      "          10       0.11      0.04      0.06        48\n",
      "          11       0.05      0.11      0.07        19\n",
      "          12       0.04      0.04      0.04        24\n",
      "\n",
      "    accuracy                           0.18       358\n",
      "   macro avg       0.18      0.19      0.18       358\n",
      "weighted avg       0.18      0.18      0.17       358\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(lr.predict(X_val), y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  0  2  0  1  0  0  0  1  1  0  3  0]\n",
      " [ 1 10  0  1  1  0  1  0  0  0  3  1  5]\n",
      " [ 0  2  3  5  3  3  1  1  0  2  3  6  0]\n",
      " [ 3  2  1  7  2  5  2  3  3  3  1  2  3]\n",
      " [ 1  0  1  1  7  2  1  1  2  1  1  3  0]\n",
      " [ 1  0  4  0  1  2  2  2  1  1  0  3  1]\n",
      " [ 2  1  2  3  2  1 11  1  0  1  3  2  0]\n",
      " [ 1  0  2  1  1  4  1  6  1  3  0  4  1]\n",
      " [ 7  3  6  4  3  2  3  1  5  5  2  2  3]\n",
      " [ 2  4  3  1  1  1  3  2  3  5  3  0  1]\n",
      " [ 4  3  4  1  6  2  5  3  6  3  2  4  5]\n",
      " [ 2  0  1  2  0  3  0  2  1  3  0  2  3]\n",
      " [ 2  1  4  3  2  1  1  2  1  1  0  5  1]]\n"
     ]
    }
   ],
   "source": [
    "print (confusion_matrix(lr.predict(X_val), y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hate          110\n",
       "anger         110\n",
       "surprise      110\n",
       "empty         110\n",
       "boredom       110\n",
       "love          110\n",
       "enthusiasm    110\n",
       "neutral       110\n",
       "sadness       110\n",
       "happiness     110\n",
       "fun           110\n",
       "worry         110\n",
       "relief        110\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>processed_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>know listenin bad habit earlier started freaki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>layin n bed headache ughhhh waitin call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>funeral ceremony gloomy friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>want hang friend soon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>want trade someone houston ticket one</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index   sentiment                                            content  \\\n",
       "0      0       empty  @tiffanylue i know  i was listenin to bad habi...   \n",
       "1      1     sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2      2     sadness                Funeral ceremony...gloomy friday...   \n",
       "3      3  enthusiasm               wants to hang out with friends SOON!   \n",
       "4      4     neutral  @dannycastillo We want to trade with someone w...   \n",
       "\n",
       "                                  processed_sentence  \n",
       "0  know listenin bad habit earlier started freaki...  \n",
       "1           layin n bed headache ughhhh waitin call   \n",
       "2                    funeral ceremony gloomy friday   \n",
       "3                             want hang friend soon   \n",
       "4             want trade someone houston ticket one   "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the String Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['String_Length'] =  df['processed_sentence'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['String_Length']>3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['String_Length'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Vader Sentiment Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\envs\\tanush_data_ops\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\DELL\\Anaconda3\\envs\\tanush_data_ops\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\DELL\\Anaconda3\\envs\\tanush_data_ops\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\DELL\\Anaconda3\\envs\\tanush_data_ops\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "## Vader Sentiment Feature Extraction\n",
    "\n",
    "df['vader_neg'] = df['processed_sentence'].apply(lambda x: sentiment_analyzer_scores(x)['neg'])\n",
    "df['vader_neu'] = df['processed_sentence'].apply(lambda x: sentiment_analyzer_scores(x)['neu'])\n",
    "df['vader_pos'] = df['processed_sentence'].apply(lambda x: sentiment_analyzer_scores(x)['pos'])\n",
    "df['vader_compound'] = df['processed_sentence'].apply(lambda x: sentiment_analyzer_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. TextBlob Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Number of Noun Phrases': 1,\n",
       " 'Number of POS tags': 6,\n",
       " 'Sentence Polarity': -0.1,\n",
       " 'Sentence Subjectivity': -0.1,\n",
       " 'Number of words': 6,\n",
       " 'Language Detected': 'en'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_textblob_features(df['content'].iloc[1002])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#try:\\ndf['Number of Noun Phrases'] = df['processed_sentence'].apply(lambda x: get_textblob_features(x)['Number of Noun Phrases'])\\ndf['Number of POS tags'] = df['processed_sentence'].apply(lambda x: get_textblob_features(x)['Number of POS tags'])\\ndf['Sentence Polarity'] = df['processed_sentence'].apply(lambda x: get_textblob_features(x)['Sentence Polarity'])\\ndf['Sentence Subjectivity'] = df['processed_sentence'].apply(lambda x: get_textblob_features(x)['Sentence Subjectivity'])\\ndf['Number of words'] = df['processed_sentence'].apply(lambda x: get_textblob_features(x)['Number of words'])\\n#except:\\n#    pass\\n\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TextBlob Feature Extraction\n",
    "'''\n",
    "#try:\n",
    "df['Number of Noun Phrases'] = df['processed_sentence'].apply(lambda x: get_textblob_features(x)['Number of Noun Phrases'])\n",
    "df['Number of POS tags'] = df['processed_sentence'].apply(lambda x: get_textblob_features(x)['Number of POS tags'])\n",
    "df['Sentence Polarity'] = df['processed_sentence'].apply(lambda x: get_textblob_features(x)['Sentence Polarity'])\n",
    "df['Sentence Subjectivity'] = df['processed_sentence'].apply(lambda x: get_textblob_features(x)['Sentence Subjectivity'])\n",
    "df['Number of words'] = df['processed_sentence'].apply(lambda x: get_textblob_features(x)['Number of words'])\n",
    "#except:\n",
    "#    pass\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob accepts strings with minimum 3 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4. Afinn Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\envs\\tanush_data_ops\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['afinn score'] = df['processed_sentence'].apply(lambda x: afinn_score(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>processed_sentence</th>\n",
       "      <th>String_Length</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>afinn score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>know listenin bad habit earlier started freaki...</td>\n",
       "      <td>53</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.5423</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>layin n bed headache ughhhh waitin call</td>\n",
       "      <td>40</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>funeral ceremony gloomy friday</td>\n",
       "      <td>31</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.4767</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>want hang friend soon</td>\n",
       "      <td>22</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>want trade someone houston ticket one</td>\n",
       "      <td>38</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index   sentiment                                            content  \\\n",
       "0      0       empty  @tiffanylue i know  i was listenin to bad habi...   \n",
       "1      1     sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2      2     sadness                Funeral ceremony...gloomy friday...   \n",
       "3      3  enthusiasm               wants to hang out with friends SOON!   \n",
       "4      4     neutral  @dannycastillo We want to trade with someone w...   \n",
       "\n",
       "                                  processed_sentence  String_Length  \\\n",
       "0  know listenin bad habit earlier started freaki...             53   \n",
       "1           layin n bed headache ughhhh waitin call              40   \n",
       "2                    funeral ceremony gloomy friday              31   \n",
       "3                             want hang friend soon              22   \n",
       "4             want trade someone houston ticket one              38   \n",
       "\n",
       "   vader_neg  vader_neu  vader_pos  vader_compound  afinn score  \n",
       "0      0.333      0.667      0.000         -0.5423         -3.0  \n",
       "1      0.000      1.000      0.000          0.0000         -2.0  \n",
       "2      0.672      0.328      0.000         -0.4767         -3.0  \n",
       "3      0.000      0.308      0.692          0.5423          2.0  \n",
       "4      0.000      0.794      0.206          0.0772          1.0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Preparing Data for Training\n",
    "#### 4.1. Encoding String/ Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>processed_sentence</th>\n",
       "      <th>String_Length</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>afinn score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>know listenin bad habit earlier started freaki...</td>\n",
       "      <td>53</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.5423</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>layin n bed headache ughhhh waitin call</td>\n",
       "      <td>40</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>funeral ceremony gloomy friday</td>\n",
       "      <td>31</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.4767</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>want hang friend soon</td>\n",
       "      <td>22</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>want trade someone houston ticket one</td>\n",
       "      <td>38</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index   sentiment                                            content  \\\n",
       "0      0       empty  @tiffanylue i know  i was listenin to bad habi...   \n",
       "1      1     sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2      2     sadness                Funeral ceremony...gloomy friday...   \n",
       "3      3  enthusiasm               wants to hang out with friends SOON!   \n",
       "4      4     neutral  @dannycastillo We want to trade with someone w...   \n",
       "\n",
       "                                  processed_sentence  String_Length  \\\n",
       "0  know listenin bad habit earlier started freaki...             53   \n",
       "1           layin n bed headache ughhhh waitin call              40   \n",
       "2                    funeral ceremony gloomy friday              31   \n",
       "3                             want hang friend soon              22   \n",
       "4             want trade someone houston ticket one              38   \n",
       "\n",
       "   vader_neg  vader_neu  vader_pos  vader_compound  afinn score  \n",
       "0      0.333      0.667      0.000         -0.5423         -3.0  \n",
       "1      0.000      1.000      0.000          0.0000         -2.0  \n",
       "2      0.672      0.328      0.000         -0.4767         -3.0  \n",
       "3      0.000      0.308      0.692          0.5423          2.0  \n",
       "4      0.000      0.794      0.206          0.0772          1.0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['index', 'sentiment', 'content', 'processed_sentence',\n",
       "       'String_Length', 'vader_neg', 'vader_neu', 'vader_pos',\n",
       "       'vader_compound', 'afinn score'], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['empty', 'sadness', 'enthusiasm', 'neutral', 'worry', 'surprise',\n",
       "       'love', 'fun', 'hate', 'happiness', 'boredom', 'relief', 'anger'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\envs\\tanush_data_ops\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(df['sentiment'])\n",
    "df['Encoded Targets'] = le.transform(df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit(df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anger', 'boredom', 'empty', 'enthusiasm', 'fun', 'happiness',\n",
       "       'hate', 'love', 'neutral', 'relief', 'sadness', 'surprise',\n",
       "       'worry'], dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\envs\\tanush_data_ops\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['Encoded Targets'] = le.transform(df['sentiment'])\n",
    "#result['Encoded Targets'] = le.transform(result['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2 10  3  8 12 11  7  4  6  5  1  9  0]\n"
     ]
    }
   ],
   "source": [
    "print (df['Encoded Targets'].unique())\n",
    "#print (result['Encoded Targets'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>processed_sentence</th>\n",
       "      <th>String_Length</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>know listenin bad habit earlier started freaki...</td>\n",
       "      <td>53</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.5423</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>layin n bed headache ughhhh waitin call</td>\n",
       "      <td>40</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>funeral ceremony gloomy friday</td>\n",
       "      <td>31</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.4767</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>want hang friend soon</td>\n",
       "      <td>22</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>want trade someone houston ticket one</td>\n",
       "      <td>38</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1012 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  index   sentiment  \\\n",
       "0        0      0       empty   \n",
       "1        1      1     sadness   \n",
       "2        2      2     sadness   \n",
       "3        3      3  enthusiasm   \n",
       "4        4      4     neutral   \n",
       "\n",
       "                                             content  \\\n",
       "0  @tiffanylue i know  i was listenin to bad habi...   \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2                Funeral ceremony...gloomy friday...   \n",
       "3               wants to hang out with friends SOON!   \n",
       "4  @dannycastillo We want to trade with someone w...   \n",
       "\n",
       "                                  processed_sentence  String_Length  \\\n",
       "0  know listenin bad habit earlier started freaki...             53   \n",
       "1           layin n bed headache ughhhh waitin call              40   \n",
       "2                    funeral ceremony gloomy friday              31   \n",
       "3                             want hang friend soon              22   \n",
       "4             want trade someone houston ticket one              38   \n",
       "\n",
       "   vader_neg  vader_neu  vader_pos  vader_compound  ...  990  991  992  993  \\\n",
       "0      0.333      0.667      0.000         -0.5423  ...    0    0    0    0   \n",
       "1      0.000      1.000      0.000          0.0000  ...    0    0    0    0   \n",
       "2      0.672      0.328      0.000         -0.4767  ...    0    0    0    0   \n",
       "3      0.000      0.308      0.692          0.5423  ...    0    0    0    0   \n",
       "4      0.000      0.794      0.206          0.0772  ...    0    0    0    0   \n",
       "\n",
       "   994  995  996  997  998  999  \n",
       "0    0    0    0    0    0    0  \n",
       "1    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    0  \n",
       "3    0    0    0    0    0    0  \n",
       "4    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 1012 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Defining Dependent and Independent Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_features = ['vader_neg', 'vader_neu', 'vader_pos', 'vader_compound']\n",
    "tf_idf_features = tf_idf_cols\n",
    "afinn_features = ['afinn score']\n",
    "target = ['Encoded Targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = result[vader_features+afinn_features]\n",
    "y = result[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample the Data Using  SMOTE and Tomek Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#93FE3F', '#9B0D6E', '#1E2A37', '#1B9B32', '#242E58', '#593568', '#2B812F', '#C94DE1']\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "number_of_colors = 8\n",
    "\n",
    "color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "             for i in range(number_of_colors)]\n",
    "print(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_2d_space(X, y, label='Classes'): \n",
    "    \n",
    "    number_of_colors = len(np.unique(y))\n",
    "\n",
    "    colors = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "                 for i in range(number_of_colors)]\n",
    "    #colors = ['#1F77B4', '#FF7F0E']\n",
    "    markers = ['o', 's']\n",
    "    for l, c, m in zip(np.unique(y), colors, markers):\n",
    "        print (l, c)\n",
    "        plt.scatter(\n",
    "            X[y==l, 0],\n",
    "            X[y==l, 1],\n",
    "            c=c, label=l\n",
    "        )\n",
    "    plt.title(label)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\envs\\tanush_data_ops\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70255 new random picked points\n",
      "(109798, 5) (39543, 5)\n",
      "0 #FAD2C8\n",
      "1 #A2FB3C\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler()\n",
    "X_ros, y_ros = ros.fit_sample(X, y)\n",
    "\n",
    "print(X_ros.shape[0] - X.shape[0], 'new random picked points')\n",
    "print (X_ros.shape, X.shape)\n",
    "\n",
    "plot_2d_space(X_ros, y_ros, 'Random over-sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12    8446\n",
       "11    8446\n",
       "10    8446\n",
       "9     8446\n",
       "8     8446\n",
       "7     8446\n",
       "6     8446\n",
       "5     8446\n",
       "4     8446\n",
       "3     8446\n",
       "2     8446\n",
       "1     8446\n",
       "0     8446\n",
       "dtype: int64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_ros).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X_result = result[tf_idf_features+vader_features+afinn_features]\n",
    "#y_result = result[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Start the Machine Learning Classification\n",
    "#### 5.1. On Vader Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_ros,y_ros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((82348, 5), (27450, 5), (82348,), (27450,))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\envs\\tanush_data_ops\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8336"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_smt.shape[0]- X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5307832422586521"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.82      0.80      1994\n",
      "           1       0.78      0.86      0.82      1940\n",
      "           2       0.47      0.62      0.54      1632\n",
      "           3       0.61      0.62      0.62      2077\n",
      "           4       0.59      0.60      0.60      2023\n",
      "           5       0.37      0.51      0.43      1522\n",
      "           6       0.73      0.75      0.74      2048\n",
      "           7       0.57      0.61      0.59      1979\n",
      "           8       0.48      0.15      0.23      6751\n",
      "           9       0.52      0.65      0.58      1755\n",
      "          10       0.40      0.55      0.46      1485\n",
      "          11       0.46      0.69      0.55      1394\n",
      "          12       0.14      0.34      0.19       850\n",
      "\n",
      "    accuracy                           0.53     27450\n",
      "   macro avg       0.53      0.60      0.55     27450\n",
      "weighted avg       0.55      0.53      0.51     27450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(rf_clf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15579783, 0.24442875, 0.1969259 , 0.25487947, 0.14796805])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1636   31   39   30   30   30   40   17   27   39   24   25   26]\n",
      " [   0 1664   35   23   18   26   35    8   29   23   28   14   37]\n",
      " [   0    0 1016   54   39   53   38   41   82   60   75   81   93]\n",
      " [   0    0   43 1294   86  103   46   89  100   84   75   75   82]\n",
      " [   0    0   22   46 1223  140   21  108  118  100   56   79  110]\n",
      " [   0    0   23   47   51  775    6  141  133   60   90   48  148]\n",
      " [   0   17   24    9   26   23 1533   19   77   33  109   38  140]\n",
      " [   0    0   23   21   46  226   16 1202  104   63   71   76  131]\n",
      " [ 463  411  850  542  447  418  289  248 1004  477  442  553  607]\n",
      " [   0    0   35   31   51  123   10   77   96 1143   53   40   96]\n",
      " [   0    0   19    0   11   50   26   52  121   27  821   59  299]\n",
      " [   0    0   13    7   43   50   28   56   62   40   39  967   89]\n",
      " [   0    0   15   16   15   67   19   36  140   29  177   44  292]]\n"
     ]
    }
   ],
   "source": [
    "print (confusion_matrix(rf_clf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. On TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res_train, X_res_test, y_res_train, y_res_test = train_test_split(X_result,y_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1065, 1005), (356, 1005), (1065, 1), (356, 1))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_res_train.shape, X_res_test.shape, y_res_train.shape, y_res_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tanush Pas\\AppData\\Local\\conda\\conda\\envs\\PDF\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Tanush Pas\\AppData\\Local\\conda\\conda\\envs\\PDF\\lib\\site-packages\\ipykernel_launcher.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.fit(X_res_train, y_res_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1404494382022472"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.score(X_res_test, y_res_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = AdaBoostClassifier(base_estimator=LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr.fit(X_res_train, y_res_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1151685393258427"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_res_test, y_res_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
