{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This program aims to classify the sentiment for the movie reviews from IMDB dataset.\n",
    "\n",
    "Data preparation:\n",
    "1) The IMDB movie review dataset:\n",
    "It can be downloaded from:\n",
    "http://ai.stanford.edu/%7Eamaas/data/sentiment/\n",
    "Unpack the downloaded IMDB package to the folder: ./aclImdb\n",
    "\n",
    "2) The GLOVE (Global Vectors for Word Representation) pretrained word vectors \n",
    "It can be downloaded from:\n",
    "http://nlp.stanford.edu/data/glove.6B.zip\n",
    "Unpack the downloaded glove package to the folder: ./glove.6B/\n",
    "\n",
    "This program uses Keras deep learning library.\n",
    "\n",
    "This program achieved an average accuracy of 0.90 over 10000 test samples. \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyPrind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Parameters and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the path to the review texts and sentiment labels\n",
    "data_path = 'data/aclImdb/'\n",
    "# the path to the glove vectors\n",
    "glove_path = 'models/glove.6B/'\n",
    "# max number of words in the texts to be vectorized (choose the frequent words)\n",
    "max_nb_words = 20000\n",
    "# max number of words in a review (the review is padded or trucated to the number)\n",
    "num_words_per_review = 1000\n",
    "# glove embedding dimension\n",
    "glove_dim = 100\n",
    "# the validation split\n",
    "validation_ratio = 0.2\n",
    "\n",
    "# fix the random seed\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Review Texts and Sentiment Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading review texts and sentiment labels ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:13:21\n"
     ]
    }
   ],
   "source": [
    "# load the movie review texts and sentiment labels\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "# there are totally 50,000 review texts\n",
    "print ('\\n')\n",
    "print ('Loading review texts and sentiment labels ...')\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(data_path, s, l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), 'r', encoding=\"utf8\") as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "            pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defiing the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['review', 'sentiment']\n",
    "texts = df['review'].values.tolist()\n",
    "labels = df['sentiment'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I went and saw this movie last night after bei...          1\n",
       "1  Actor turned director Bill Paxton follows up h...          1\n",
       "2  As a recreational golfer with some knowledge o...          1\n",
       "3  I saw this film in a sneak preview, and it is ...          1\n",
       "4  Bill Paxton has taken the true story of the 19...          1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading GLOVE Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GLOVE word vectors ...\n"
     ]
    }
   ],
   "source": [
    "# load the glove vectors\n",
    "print ('Loading GLOVE word vectors ...')\n",
    "# the dictionary for maping a word to a 100-dim vector\n",
    "glove_embedding = {}\n",
    "f = open(os.path.join(glove_path, 'glove.6B.100d.txt'), encoding=\"utf8\")\n",
    "for line in f:\n",
    "    fields = line.split()\n",
    "    word = fields[0] # the first element is the word\n",
    "    word_vector = np.asarray(fields[1:], dtype='float32') \n",
    "    glove_embedding[word] = word_vector\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#f=open('dictionaries/glove_embedding_v1.pickle','wb')\n",
    "#pickle.dump(glove_embedding,f)\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f1 = open('dictionaries/glove_embedding_v1.pickle', 'rb')\n",
    "glove_embedding = pickle.load(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tanush Pas\\AppData\\Local\\conda\\conda\\envs\\PDF\\lib\\site-packages\\keras_preprocessing\\text.py:178: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "# tokenize the words in the texts\n",
    "tokenizer = Tokenizer(nb_words = max_nb_words) \n",
    "tokenizer.fit_on_texts(texts) \n",
    "# convert each review text into a sequence of word-indices\n",
    "matrix_word_indices = tokenizer.texts_to_sequences(texts)\n",
    "# the dictionary for mapping a word to an index\n",
    "dictionary_word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad each review text to a fixed length of word sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_word_indices_fixed_length = pad_sequences(matrix_word_indices, maxlen = num_words_per_review)\n",
    "# convert to numpy arrays \n",
    "data = np.array(matrix_word_indices_fixed_length)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "# percentage of validation data\n",
    "nb_validation_samples = int(validation_ratio*data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocation of training data and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_validation = data[-nb_validation_samples:]\n",
    "y_validation = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing the words ...\n"
     ]
    }
   ],
   "source": [
    "num_words = min(max_nb_words, len(dictionary_word_index))\n",
    "# embedding_matrix[0] is a all-zero vector representing no word\n",
    "embedding_matrix = np.zeros((num_words+1, glove_dim)) \n",
    "print ('Vectorizing the words ...')\n",
    "for word, index in dictionary_word_index.items():\n",
    "    if index > max_nb_words:\n",
    "        continue \n",
    "    # get the glove vector for the word\n",
    "    glove_vector = glove_embedding.get(word) \n",
    "    if glove_vector is not None: \n",
    "        embedding_matrix[index] = glove_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tanush Pas\\AppData\\Local\\conda\\conda\\envs\\PDF\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=128, kernel_size=5)`\n",
      "  \n",
      "C:\\Users\\Tanush Pas\\AppData\\Local\\conda\\conda\\envs\\PDF\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=5)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Tanush Pas\\AppData\\Local\\conda\\conda\\envs\\PDF\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=5)`\n",
      "  \n",
      "C:\\Users\\Tanush Pas\\AppData\\Local\\conda\\conda\\envs\\PDF\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n",
      "C:\\Users\\Tanush Pas\\AppData\\Local\\conda\\conda\\envs\\PDF\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n"
     ]
    }
   ],
   "source": [
    "# layer 0: the input layer\n",
    "sequence_input = Input(shape=(num_words_per_review,), dtype='int32')\n",
    "# layer-1: the embedding layer\n",
    "embedding_layer = Embedding(num_words+1, glove_dim, weights=[embedding_matrix], input_length=num_words_per_review, trainable=True)\n",
    "embedded_output = embedding_layer(sequence_input)\n",
    "# layer-2: the first convolution layer\n",
    "x = Conv1D(nb_filter=128, filter_length=5, activation='relu')(embedded_output)\n",
    "# layer-3: the first pooling layer\n",
    "x = MaxPooling1D(pool_length=5)(x)\n",
    "# layer-4: the second convolution layer\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "# layer-5: the second pooling layer\n",
    "x = MaxPooling1D(pool_length = 5)(x)\n",
    "# flatten layer\n",
    "x = Flatten()(x)\n",
    "# layer-6: the first dense layer\n",
    "x = Dense(output_dim = 128, activation='relu')(x)\n",
    "# layer-7: the second dense layer\n",
    "x = Dense(output_dim = 128, activation='relu')(x)\n",
    "# layer-8: the output layer\n",
    "final_output = Dense(1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tanush Pas\\AppData\\Local\\conda\\conda\\envs\\PDF\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Model(input=sequence_input, output=final_output)\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[    0,     0,     0, ...,  5200,   266,   268],\n",
       "        [    0,     0,     0, ...,     7,   230,   157],\n",
       "        [    0,     0,     0, ...,  3440,     2, 17886],\n",
       "        ...,\n",
       "        [    0,     0,     0, ...,     7,     7,  1017],\n",
       "        [    0,     0,     0, ...,   532,    42,     9],\n",
       "        [    0,     0,     0, ...,    43,    21,    89]]),\n",
       " array([1, 0, 1, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tanush Pas\\AppData\\Local\\conda\\conda\\envs\\PDF\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "40000/40000 [==============================] - 1307s 33ms/step - loss: 0.6251 - acc: 0.6632 - val_loss: 0.4268 - val_acc: 0.8188\n",
      "Epoch 2/5\n",
      "40000/40000 [==============================] - 1275s 32ms/step - loss: 0.3839 - acc: 0.8360 - val_loss: 0.2891 - val_acc: 0.8782\n",
      "Epoch 3/5\n",
      "40000/40000 [==============================] - 1293s 32ms/step - loss: 0.3041 - acc: 0.8765 - val_loss: 0.2995 - val_acc: 0.8730\n",
      "Epoch 4/5\n",
      "40000/40000 [==============================] - 1265s 32ms/step - loss: 0.2644 - acc: 0.8945 - val_loss: 0.2867 - val_acc: 0.8892\n",
      "Epoch 5/5\n",
      "40000/40000 [==============================] - 1296s 32ms/step - loss: 0.2441 - acc: 0.9065 - val_loss: 0.2462 - val_acc: 0.9008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11b692b3d68>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training and validation\n",
    "print ('Training the model ...')\n",
    "model.fit(x=x_train, y=y_train, validation_data=(x_validation, y_validation), nb_epoch=5, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model ...\n",
      "10000/10000 [==============================] - 60s 6ms/step\n",
      "\n",
      "The average accuracy on the evaluation data set is 0.901.\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "print ('Evaluating the model ...')\n",
    "test_accuracy = model.evaluate(x_validation, y_validation, verbose=1)\n",
    "print ('\\nThe average accuracy on the evaluation data set is %.3f.' % test_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/keras_custom_90_perc.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
